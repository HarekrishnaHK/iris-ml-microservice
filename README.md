
```markdown
# ğŸ§  ML Pipeline with Flask, Kafka, gRPC, and SQLite

This project demonstrates a Machine Learning pipeline using:
- Flask API for receiving prediction input
- Kafka for message streaming
- gRPC microservice for saving results
- SQLite for persistent storage
- Classification and Regression ML models (Iris dataset)

## ğŸ“Œ Project Structure

```

ml\_pipeline/
â”œâ”€â”€ flask\_api.py              # Input validation + Kafka publisher
â”œâ”€â”€ publisher.py              # Standalone Kafka message publisher
â”œâ”€â”€ subscriber1.py            # Kafka subscriber for classification
â”œâ”€â”€ subscriber2.py            # Kafka subscriber for regression
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ iris\_logistic\_model.pkl
â”‚   â”œâ”€â”€ iris\_regression\_model.pkl
â”‚   â””â”€â”€ iris\_label\_encoder.pkl
â”œâ”€â”€ grpc/
â”‚   â”œâ”€â”€ store.proto           # gRPC service definition
â”‚   â”œâ”€â”€ store\_pb2.py          # Generated gRPC classes
â”‚   â”œâ”€â”€ store\_pb2\_grpc.py
â”‚   â”œâ”€â”€ store\_server.py       # gRPC server
â”‚   â””â”€â”€ store\_client.py       # Client to send gRPC requests
â”œâ”€â”€ store\_results.db          # SQLite database
â””â”€â”€ init\_db.py                # DB setup script

```

---

## ğŸš€ Setup Instructions
```
## 1. ğŸ”§ Install Requirements

```bash
pip install flask joblib scikit-learn confluent-kafka grpcio grpcio-tools
````
```
### 2. âš™ï¸ Start Kafka (using Confluent CLI)

```bash
confluent local kafka start
confluent local kafka topic create manage
```

---

### 3. ğŸ“¡ Run gRPC Server

```bash
python store_server.py
```

---

### 4. ğŸ§ª Run Subscribers (in separate terminals)

 **Classification Subscriber**

```bash
python subscriber1.py
```

 **Regression Subscriber**

```bash
python subscriber2.py
```

---

### 5. ğŸŒ Run Flask API (Publisher)

```bash
python server.py
```

Then send a POST request:

```json
POST http://127.0.0.1:5001/predict

{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}
```

âœ… This triggers Kafka publishing â†’ subscribers â†’ gRPC calls â†’ database insertion.

---

## ğŸ—ƒï¸ Database Tables

The SQLite database `store_results.db` has two tables:

### classification

| id | model\_name      | features                | predicted\_label |
| -- | ---------------- | ----------------------- | ---------------- |
| 1  | iris\_classifier | "\[5.1, 3.5, 1.4, 0.2]" | setosa           |

### regression

| id | model\_name     | features           | predicted\_value |
| -- | --------------- | ------------------ | ---------------- |
| 1  | iris\_regressor | "\[5.1, 3.5, 1.4]" | 0.2095           |

---

## ğŸ“¦ Proto Definition

```proto
// store.proto

service StoreService {
  rpc SaveRegress (RegressRequest) returns (RegressResponse);
  rpc SaveClassify (ClassifyRequest) returns (ClassifyResponse);
}

message RegressRequest {
  string model_name = 1;
  string features = 2;
  float predicted_value = 3;
}

message RegressResponse {
  string status = 1;
}

message ClassifyRequest {
  string model_name = 1;
  string features = 2;
  string predicted_label = 3;
}

message ClassifyResponse {
  string status = 1;
}
```

---

## ğŸ’¡ Features

* âœ… Real-time classification & regression
* âœ… Kafka for stream-based processing
* âœ… gRPC microservices for clean architecture
* âœ… SQLite for easy result persistence
* âœ… Input validation and preprocessing

---

## ğŸ“š Future Improvements

* Add Docker support for full deployment
* Add authentication to the gRPC service
* Switch to PostgreSQL or MySQL for production use

---

## ğŸ‘¨â€ğŸ’» Author

**Harekrishna Adhikary**
ğŸ“ B.Tech CSE (Data Science) | The Neotia University
ğŸ§  Focus: ML, Deep Learning, Microservices
ğŸ“« *Drop a message if you'd like to collaborate!*
---