# ğŸ§  ML Pipeline with Flask, Kafka, gRPC, and SQLite

This project demonstrates a Machine Learning pipeline using:
- **Flask API** for receiving prediction input
- **Kafka** for message streaming
- **gRPC microservice** for saving results
- **SQLite** for persistent storage
- **Classification** and **Regression** ML models (Iris dataset)



## ğŸ“Œ Project Structure



ml\_pipeline/
â”œâ”€â”€ flask\_api.py              # Input validation + Kafka publisher
â”œâ”€â”€ publisher.py              # Standalone Kafka message publisher
â”œâ”€â”€ subscriber1.py            # Kafka subscriber for classification
â”œâ”€â”€ subscriber2.py            # Kafka subscriber for regression
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ iris\_logistic\_model.pkl
â”‚   â”œâ”€â”€ iris\_regression\_model.pkl
â”‚   â””â”€â”€ iris\_label\_encoder.pkl
â”œâ”€â”€ grpc/
â”‚   â”œâ”€â”€ store.proto           # gRPC service definition
â”‚   â”œâ”€â”€ store\_pb2.py          # Generated gRPC classes
â”‚   â”œâ”€â”€ store\_pb2\_grpc.py
â”‚   â”œâ”€â”€ store\_server.py       # gRPC server
â”‚   â””â”€â”€ store\_client.py       # Client to send gRPC requests
â”œâ”€â”€ store\_results.db          # SQLite database
â””â”€â”€ init\_db.py                # DB setup script



ğŸš€ Setup Instructions

1. ğŸ”§ Install Requirements


pip install flask joblib scikit-learn confluent-kafka grpcio grpcio-tools

2. âš™ï¸ Start Kafka (using Confluent CLI)


confluent local services start
confluent local kafka topic create manage

3. ğŸ“¡ Run gRPC Server

```bash
python grpc/store_server.py
```

---

### 4. ğŸ§ª Run Subscribers (in separate terminals)

* **Classification Subscriber**

```bash
python subscriber1.py
```

* **Regression Subscriber**

```bash
python subscriber2.py
```

---

### 5. ğŸŒ Run Flask API (Publisher)

```bash
python flask_api.py
```

Then send a POST request:

```json
POST http://127.0.0.1:5001/predict

{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}


âœ… This triggers Kafka publishing â†’ subscribers â†’ gRPC calls â†’ database insertion.

---

## ğŸ—ƒï¸ Database Tables

The SQLite database `store_results.db` has two tables:

### classification

| id | model\_name      | features                | predicted\_label |
| -- | ---------------- | ----------------------- | ---------------- |
| 1  | iris\_classifier | "\[5.1, 3.5, 1.4, 0.2]" | setosa           |

### regression

| id | model\_name     | features           | predicted\_value |
| -- | --------------- | ------------------ | ---------------- |
| 1  | iris\_regressor | "\[5.1, 3.5, 1.4]" | 0.2095           |

---

## ğŸ“¦ Proto Definition

```proto
// store.proto

service StoreService {
  rpc SaveRegress (RegressRequest) returns (RegressResponse);
  rpc SaveClassify (ClassifyRequest) returns (ClassifyResponse);
}

message RegressRequest {
  string model_name = 1;
  string features = 2;
  float predicted_value = 3;
}

message RegressResponse {
  string status = 1;
}

message ClassifyRequest {
  string model_name = 1;
  string features = 2;
  string predicted_label = 3;
}

message ClassifyResponse {
  string status = 1;
}
```

---

## ğŸ’¡ Features

* âœ… Real-time classification & regression
* âœ… Kafka for stream-based processing
* âœ… gRPC microservices for clean architecture
* âœ… SQLite for easy result persistence
* âœ… Input validation and preprocessing

---

## ğŸ“š Future Improvements

* Add Docker support for full deployment
* Add authentication to the gRPC service
* Switch to PostgreSQL or MySQL for production use

---

## ğŸ‘¨â€ğŸ’» Author

**Harekrishna Adhikary**
B.Tech in CSE (Data Science)
[The Neotia University]







Here's a **beautifully designed and unique** `README.md` using **modern markdown**, emojis, icons, and a visually structured layout to stand out:

---

```markdown
<h1 align="center">ğŸš€ ML Streaming Pipeline</h1>
<h3 align="center">Flask API + Kafka + gRPC + SQLite + Machine Learning</h3>

<p align="center">
  <img src="https://img.shields.io/badge/ML-Classification%20%26%20Regression-brightgreen" />
  <img src="https://img.shields.io/badge/Streaming-Kafka-blue" />
  <img src="https://img.shields.io/badge/API-Flask-orange" />
  <img src="https://img.shields.io/badge/gRPC-Service-lightgrey" />
  <img src="https://img.shields.io/badge/Database-SQLite-purple" />
</p>

---

## ğŸ§© Overview

A modular Machine Learning microservice system featuring:

âœ¨ **Flask** for input API  
ğŸ“¡ **Kafka** for real-time message passing  
ğŸ”Œ **gRPC** service for storing results  
ğŸ“ˆ **ML Models** (Classification & Regression)  
ğŸ›¢ï¸ **SQLite DB** for persistent storage

---

## ğŸ“‚ Project Layout

```

ml\_pipeline/
â”‚
â”œâ”€â”€ flask\_api.py               â†’ Input validation & Kafka publisher
â”œâ”€â”€ subscriber1.py             â†’ Classification consumer
â”œâ”€â”€ subscriber2.py             â†’ Regression consumer
â”œâ”€â”€ publisher.py               â†’ Sample Kafka publisher
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ iris\_logistic\_model.pkl
â”‚   â”œâ”€â”€ iris\_regression\_model.pkl
â”‚   â””â”€â”€ iris\_label\_encoder.pkl
â”‚
â”œâ”€â”€ grpc/
â”‚   â”œâ”€â”€ store.proto
â”‚   â”œâ”€â”€ store\_pb2.py
â”‚   â”œâ”€â”€ store\_pb2\_grpc.py
â”‚   â”œâ”€â”€ store\_server.py        â†’ gRPC server
â”‚   â””â”€â”€ store\_client.py        â†’ gRPC client
â”‚
â”œâ”€â”€ init\_db.py                 â†’ SQLite DB table setup
â””â”€â”€ store\_results.db           â†’ Local database

````

---

## ğŸ”§ Setup Guide

### ğŸ Install Dependencies
```bash
pip install flask joblib scikit-learn confluent-kafka grpcio grpcio-tools
````

### ğŸ§± Start Kafka Broker (Confluent CLI)

```bash
confluent local services start
confluent local kafka topic create manage
```

---

## â–¶ï¸ Run The Services

> Open **separate terminals** for each step below ğŸ‘‡

### 1ï¸âƒ£ gRPC Server

```bash
python grpc/store_server.py
```

### 2ï¸âƒ£ Subscribers

* **Classification**

  ```bash
  python subscriber1.py
  ```

* **Regression**

  ```bash
  python subscriber2.py
  ```

### 3ï¸âƒ£ Flask API

```bash
python flask_api.py
```

Then, send a POST request using Postman or `curl`:

```json
POST http://127.0.0.1:5001/predict

{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}
```

---

## ğŸ§  ML Model Info

| Type           | Model               | Features Used                   | Output                   |
| -------------- | ------------------- | ------------------------------- | ------------------------ |
| Classification | Logistic Regression | All 4 features                  | Iris species (label)     |
| Regression     | Linear Regression   | 3 features (sepal/petal length) | Predicted numeric target |

---

## ğŸ—ƒï¸ Database Schema

> SQLite database `store_results.db` will be automatically populated.

**Table: `classification`**

| id | model\_name      | features                | predicted\_label |
| -- | ---------------- | ----------------------- | ---------------- |
| 1  | iris\_classifier | "\[5.1, 3.5, 1.4, 0.2]" | setosa           |

**Table: `regression`**

| id | model\_name     | features           | predicted\_value |
| -- | --------------- | ------------------ | ---------------- |
| 1  | iris\_regressor | "\[5.1, 3.5, 1.4]" | 0.2095           |

---

## ğŸ“¡ gRPC Proto File

```proto
service StoreService {
  rpc SaveRegress (RegressRequest) returns (RegressResponse);
  rpc SaveClassify (ClassifyRequest) returns (ClassifyResponse);
}

message RegressRequest {
  string model_name = 1;
  string features = 2;
  float predicted_value = 3;
}

message ClassifyRequest {
  string model_name = 1;
  string features = 2;
  string predicted_label = 3;
}
```

---

## ğŸ§ª Sample Kafka Publisher (Optional)

```bash
python publisher.py
```

---

## ğŸ’¡ Future Ideas

* ğŸ³ Dockerize all components
* ğŸ›¡ Add gRPC authentication
* ğŸ“Š Add Grafana + Prometheus monitoring
* ğŸ§ª Integrate unit testing and logging

---

## ğŸ‘¨â€ğŸ’» Author

**Harekrishna Adhikary**
ğŸ“ B.Tech CSE (Data Science) | The Neotia University
ğŸ§  Focus: ML, Deep Learning, Microservices
ğŸ“« *Drop a message if you'd like to collaborate!*

---

> Made with â¤ï¸ and Python
